{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3860838,"sourceType":"datasetVersion","datasetId":2295895},{"sourceId":8221480,"sourceType":"datasetVersion","datasetId":4874035},{"sourceId":8432556,"sourceType":"datasetVersion","datasetId":5022133}],"dockerImageVersionId":30684,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nfrom nltk.tokenize import  word_tokenize\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm #for the \nimport statistics            #Normal curve\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-18T19:55:23.423523Z","iopub.execute_input":"2024-05-18T19:55:23.424030Z","iopub.status.idle":"2024-05-18T19:55:27.252168Z","shell.execute_reply.started":"2024-05-18T19:55:23.423990Z","shell.execute_reply":"2024-05-18T19:55:27.249557Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/xecdata/Crypto/bitcoin_2013-12-28_2024-04-24.csv\n/kaggle/input/xecdata/Crypto/ethereum_2013-12-28_2024-04-24.csv\n/kaggle/input/xecdata/Crypto/monero_2013-12-28_2024-04-24.csv\n/kaggle/input/bitcoin-tweets/bitcoin_tweets.csv\n/kaggle/input/youknow/negative-words.csv\n/kaggle/input/youknow/positive-words.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"#data\n#everyday from 2017-11-09 to 2024-04-15 & is in reverse order\nbit = pd.read_csv('/kaggle/input/xecdata/Crypto/bitcoin_2013-12-28_2024-04-24.csv')\neth = pd.read_csv('/kaggle/input/xecdata/Crypto/ethereum_2013-12-28_2024-04-24.csv')\nxmr = pd.read_csv('/kaggle/input/xecdata/Crypto/monero_2013-12-28_2024-04-24.csv')\ndata = pd.read_csv('/kaggle/input/bitcoin-tweets/bitcoin_tweets.csv')\nNEmoWords = pd.read_csv('/kaggle/input/youknow/negative-words.csv', encoding='latin-1')\nPEmoWords = pd.read_csv('/kaggle/input/youknow/positive-words.csv', encoding='latin-1')","metadata":{"execution":{"iopub.status.busy":"2024-05-18T19:55:27.255868Z","iopub.execute_input":"2024-05-18T19:55:27.256968Z","iopub.status.idle":"2024-05-18T19:55:39.264662Z","shell.execute_reply.started":"2024-05-18T19:55:27.256911Z","shell.execute_reply":"2024-05-18T19:55:39.263094Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Notes from viewing data:\n1.     The price always falls after a peak\n        ","metadata":{}},{"cell_type":"markdown","source":"Notes from collecting information:\n1. I might need to go on social media platforms like youtube, twitter,\n     and reddit to get more accurate reactions and buys from people\n2. I've done that, so now i need to tokenize the tweets","metadata":{}},{"cell_type":"code","source":"enOnly = data.loc[data['lang'] == 'en']","metadata":{"execution":{"iopub.status.busy":"2024-05-18T19:55:39.266177Z","iopub.execute_input":"2024-05-18T19:55:39.266513Z","iopub.status.idle":"2024-05-18T19:55:39.645947Z","shell.execute_reply.started":"2024-05-18T19:55:39.266485Z","shell.execute_reply":"2024-05-18T19:55:39.644276Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"positive_words = set(PEmoWords['Positive Words'])\nnegative_words = set(NEmoWords['Negative Words'])\n\ndef tokenize_tweet(tweet):\n    return word_tokenize(tweet.lower())\n\ndef detect_sentiment_words(tokens):\n    positive_count = sum(1 for word in tokens if word in positive_words)\n    negative_count = sum(1 for word in tokens if word in negative_words)\n    return positive_count, negative_count\n\ndef process_tweets(tweets):\n    data = []\n    for tweet in tweets:\n        tokens = tokenize_tweet(tweet)\n        pos_count, neg_count = detect_sentiment_words(tokens)\n        data.append({'tweet': tweet, 'positive_count': pos_count, 'negative_count': neg_count})\n    return pd.DataFrame(data)\n\ntest = process_tweets(enOnly['text'])","metadata":{"execution":{"iopub.status.busy":"2024-05-18T19:55:39.650065Z","iopub.execute_input":"2024-05-18T19:55:39.651046Z","iopub.status.idle":"2024-05-18T20:03:53.702183Z","shell.execute_reply.started":"2024-05-18T19:55:39.650994Z","shell.execute_reply":"2024-05-18T20:03:53.700108Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m: tweet, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive_count\u001b[39m\u001b[38;5;124m'\u001b[39m: pos_count, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative_count\u001b[39m\u001b[38;5;124m'\u001b[39m: neg_count})\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[0;32m---> 20\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43menOnly\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[4], line 15\u001b[0m, in \u001b[0;36mprocess_tweets\u001b[0;34m(tweets)\u001b[0m\n\u001b[1;32m     13\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m tweets:\n\u001b[0;32m---> 15\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_tweet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtweet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     pos_count, neg_count \u001b[38;5;241m=\u001b[39m detect_sentiment_words(tokens)\n\u001b[1;32m     17\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m: tweet, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive_count\u001b[39m\u001b[38;5;124m'\u001b[39m: pos_count, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative_count\u001b[39m\u001b[38;5;124m'\u001b[39m: neg_count})\n","Cell \u001b[0;32mIn[4], line 5\u001b[0m, in \u001b[0;36mtokenize_tweet\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_tweet\u001b[39m(tweet):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtweet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/tokenize/__init__.py:131\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m:type preserver_line: bool\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/tokenize/__init__.py:132\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m:type preserver_line: bool\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences\n\u001b[0;32m--> 132\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/tokenize/treebank.py:136\u001b[0m, in \u001b[0;36mTreebankWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    133\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(substitution, text)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS2:\n\u001b[0;32m--> 136\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mregexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m1 \u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m2 \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS3:\n\u001b[1;32m    138\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2 \u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"def FNS(series, name):\n    min = np.min(series)\n    q1 = np.quantile(series, .25)\n    med = np.quantile(series, .50)\n    q3 = np.quantile(series, .75)\n    max = np.max(series)\n    \n    print(name, \"5-num-sum\")\n    print(\"min:\", min)\n    print(\"Q1:\", q1)\n    print(\"med:\", med)\n    print(\"Q3:\", q3)\n    print(\"max:\", max, \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T20:03:53.703893Z","iopub.status.idle":"2024-05-18T20:03:53.707372Z","shell.execute_reply.started":"2024-05-18T20:03:53.707002Z","shell.execute_reply":"2024-05-18T20:03:53.707038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Bhigh = bit.High\nBH_FNS = FNS(Bhigh, \"Bitcoin High:\")\n# Blow = bit.Low\n# BL_FNS = FNS(Blow, \"Bitcoin Low:\")\n\nXhigh = xmr.High\nXH_FNS = FNS(Xhigh, \"Monero High:\")\n# Xlow = doge.Low\n# XH_FNS = FNS(XLow, \"Dogecoin Low:\")\n\nEhigh = eth.High\nEH_FNS = FNS(Ehigh, \"Etherium High:\")\n# Ehigh = eth.Low\n# EH_FNS = FNS(Ehigh, \"Etherium Low:\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-18T20:03:53.710871Z","iopub.status.idle":"2024-05-18T20:03:53.711401Z","shell.execute_reply.started":"2024-05-18T20:03:53.711190Z","shell.execute_reply":"2024-05-18T20:03:53.711210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Bifneg = bit.High - bit.Low\nXifneg = xmr.High #- xmr.Low\nEifneg = eth.High - eth.Low\n#plt.figure(figsize=(500,500))\nplt.xlabel(\"Days{2013 to 2024}\")\nplt.ylabel(\"Dollars\")\n#Eifneg.plot(color='blue')\n#Bifneg.plot(color='orange')\nXifneg.plot(color='black')\n#bit.Volume.plot()\n#eth.Volume.plot()","metadata":{"execution":{"iopub.status.busy":"2024-05-18T20:03:53.712883Z","iopub.status.idle":"2024-05-18T20:03:53.713276Z","shell.execute_reply.started":"2024-05-18T20:03:53.713089Z","shell.execute_reply":"2024-05-18T20:03:53.713105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', None)\nfirst_peak = bit[1200:1500]\nsecond_peak = bit[1900:2100] \nthird_peak = bit[2480:2750]\nfourth_peak = bit[2750:3000]\nfith_peak = bit[3540:]\n\nplt.xlabel(\"Days{2013 to 2024}\")\nplt.ylabel(\"Dollars\")\n\nbit.High.plot(color='green')\nfirst_peak.High.plot(color='orange')\nsecond_peak.High.plot(color='orange')\nthird_peak.High.plot(color='orange')\nfourth_peak.High.plot(color='orange')\nfith_peak.High.plot(color='orange')","metadata":{"execution":{"iopub.status.busy":"2024-05-18T20:03:53.714421Z","iopub.status.idle":"2024-05-18T20:03:53.714975Z","shell.execute_reply.started":"2024-05-18T20:03:53.714675Z","shell.execute_reply":"2024-05-18T20:03:53.714697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FNS(first_peak.High, \"First Peak\")\n# FNS(second_peak.High, \"Second Peak\")\n# FNS(third_peak.High, \"Third Peak\")\n# FNS(fourth_peak.High, \"Fouth Peak\")\n# FNS(fith_peak.High, \"Fith Peak\")\n\n#bit[]","metadata":{"execution":{"iopub.status.busy":"2024-05-18T20:03:53.716700Z","iopub.status.idle":"2024-05-18T20:03:53.717250Z","shell.execute_reply.started":"2024-05-18T20:03:53.716988Z","shell.execute_reply":"2024-05-18T20:03:53.717010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fourth_peak.plot.scatter(x='Start', y='High',c='orange')","metadata":{"execution":{"iopub.status.busy":"2024-05-18T20:03:53.720122Z","iopub.status.idle":"2024-05-18T20:03:53.721948Z","shell.execute_reply.started":"2024-05-18T20:03:53.721614Z","shell.execute_reply":"2024-05-18T20:03:53.721658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}